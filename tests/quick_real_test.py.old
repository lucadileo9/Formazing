"""
Quick Real Test - Test interattivo per componenti reali usando fixture pytest

SCOPO:
- Test veloce di tutti i componenti reali
- Interattivo: chiede cosa testare  
- Perfetto per debug e sviluppo
- Usa fixture centralizzate da conftest.py

USAGE:
    pytest tests/quick_real_test.py::test_quick_interactive -s
    oppure
    python -m pytest tests/quick_real_test.py::test_quick_interactive -s
    
FEATURES:
- Test formattazione senza invio (sempre sicuro)
- Invio notifiche formazione reali (opzionale, chiede conferma)
- Invio notifiche feedback reali (opzionale, chiede conferma)
- Test comandi bot interattivi (opzionale)  
- Diagnostica completa sistema
- Usa fixture pytest per configurazione automatica
"""

import asyncio
import pytest
from datetime import datetime


class QuickTestRunner:
    """Runner per test rapidi interattivi usando fixture pytest."""
    
    def __init__(self, telegram_service, notion_service, sample_data):
        """
        Inizializza runner con servizi dalle fixture.
        
        Args:
            telegram_service: TelegramService configurato (da fixture)
            notion_service: MockNotionService (da fixture)  
            sample_data: Dati di esempio (da fixture)
        """
        self.service = telegram_service
        self.mock_notion = notion_service
        self.sample_data = sample_data
        
        print("🔧 Setup sistema di test...")
        print(f"✅ Sistema configurato con {len(self.service.groups)} gruppi di test")
        print("✅ Mock Notion service configurato dalle fixture")
    
    async def test_1_formatting_only(self):
        """Test 1: Solo formattazione (sicuro, no invio)."""
        print("\n" + "="*50)
        print("🎨 TEST 1: FORMATTAZIONE (no invio)")
        print("="*50)
        
        training_data = self.sample_data
        print(f"📊 Dati test: {training_data['Nome']} - {training_data['Area']}")
        
        try:
            # Test formattazione training
            main_msg = self.service.formatter.format_training_message(training_data, 'main_group')
            area_msg = self.service.formatter.format_training_message(training_data, 'area_group')
            
            # Test formattazione feedback
            feedback_msg = self.service.formatter.format_feedback_message(
                training_data, 
                'https://forms.office.com/test-feedback',
                'message'
            )
            
            print(f"\n📄 TRAINING MAIN MESSAGE (primi 150 char):")
            print(f"📝 {main_msg[:150]}...")
            
            print(f"\n📄 TRAINING AREA MESSAGE (primi 150 char):")
            print(f"📝 {area_msg[:150]}...")
            
            print(f"\n📄 FEEDBACK MESSAGE (primi 150 char):")
            print(f"📝 {feedback_msg[:150]}...")
            
            # Verifica marker [TEST]
            messages = [main_msg, area_msg, feedback_msg]
            all_have_test = all('[TEST]' in msg for msg in messages)
            
            if all_have_test:
                print(f"\n✅ Formattazione OK - Tutti i messaggi hanno marker [TEST]")
                return True
            else:
                print(f"\n⚠️ Attenzione: Marker [TEST] mancante in alcuni messaggi")
                return False
                
        except Exception as e:
            print(f"\n❌ Errore formattazione: {e}")
            return False
    
    async def test_2_send_real_notification(self):
        """Test 2: Invio notifica reale (opzionale)."""
        print("\n" + "="*50)
        print("📤 TEST 2: INVIO NOTIFICA REALE")
        print("="*50)
        
        print("⚠️  ATTENZIONE: Questo test invierà messaggi REALI sui gruppi Telegram!")
        print("📱 I messaggi saranno inviati sui gruppi configurati in test_telegram_groups.json")
        print("🔍 I messaggi saranno marcati [TEST] per identificarli")
        
        confirm = input("\n❓ Vuoi continuare con l'invio reale? (y/N): ").strip().lower()
        
        if confirm not in ['y', 'yes', 's', 'si']:
            print("⏭️ Invio notifica saltato")
            return True
        
        # Usa dati dalle fixture con modifiche per invio reale
        training_data = self.sample_data.copy()
        training_data['Nome'] = f'Real Test Course {datetime.now().strftime("%H:%M")}'
        training_data['Codice'] = f'REAL{datetime.now().strftime("%H%M")}'
        training_data['Descrizione'] = 'Test reale invio notifica'
        training_data['Docente'] = 'Real Test Bot'
        
        print(f"\n📤 Invio notifica per: {training_data['Nome']}")
        print(f"🎯 Area target: {training_data['Area']}")
        
        try:
            results = await self.service.send_training_notification(training_data)
            
            successful = sum(1 for success in results.values() if success)
            total = len(results)
            
            print(f"\n✅ Notifica inviata a {successful}/{total} gruppi")
            print(f"📋 Risultati dettagli:")
            for group, success in results.items():
                status = "✅" if success else "❌"
                print(f"   {status} {group}")
            
            print(f"\n🔍 Controlla i gruppi Telegram per vedere i messaggi!")
            return successful > 0
            
        except Exception as e:
            print(f"\n❌ Errore invio: {e}")
            return False
    
    async def test_3_bot_commands(self):
        """Test 3: Comandi bot interattivi."""
        print("\n" + "="*50)
        print("🤖 TEST 3: COMANDI BOT INTERATTIVI")
        print("="*50)
        
        # Info dati mock
        mock_info = self.mock_notion.get_current_test_info()
        print(f"📊 Mock Notion configurato:")
        print(f"   📅 Data base: {mock_info['base_date']}")
        print(f"   📋 Formazioni oggi: {mock_info['today_formazioni']}")
        print(f"   📋 Formazioni domani: {mock_info['tomorrow_formazioni']}")
        print(f"   📋 Formazioni settimana: {mock_info['week_formazioni']}")
        
        print(f"\n⚠️  Questo test avvierà il bot per 30 secondi")
        print(f"📱 Potrai testare manualmente i comandi su Telegram")
        
        test_commands = input(f"\n❓ Vuoi testare i comandi bot? (y/N): ").strip().lower()
        
        if test_commands not in ['y', 'yes', 's', 'si']:
            print("⏭️ Test comandi saltato")
            return True
        
        print(f"\n🚀 Avvio bot per test comandi...")
        print(f"💡 Comandi disponibili:")
        print(f"   /oggi - Mostra {mock_info['today_formazioni']} formazioni di oggi")
        print(f"   /domani - Mostra {mock_info['tomorrow_formazioni']} formazione di domani")
        print(f"   /settimana - Mostra tutte le formazioni della settimana")
        print(f"   /help - Guida comandi")
        print(f"\n⏰ Bot attivo per 30 secondi - CTRL+C per fermare prima")
        
        try:
            await self.service.start_bot()
            
            # Countdown visivo
            for i in range(30, 0, -5):
                print(f"⏰ Bot attivo - {i} secondi rimanenti...")
                await asyncio.sleep(5)
            
        except KeyboardInterrupt:
            print(f"\n⌨️ Test interrotto dall'utente")
        except Exception as e:
            print(f"\n❌ Errore bot: {e}")
            return False
        finally:
            await self.service.stop_bot()
            print(f"✅ Bot fermato")
        
        return True
    
    async def test_4_mock_data_diagnostic(self):
        """Test 4: Diagnostica dati mock."""
        print("\n" + "="*50)
        print("🔍 TEST 4: DIAGNOSTICA DATI MOCK")
        print("="*50)
        
        try:
            # Test recupero dati
            formazioni_cal = await self.mock_notion.get_formazioni_by_status('Calendarizzata')
            formazioni_comp = await self.mock_notion.get_formazioni_by_status('Completata')
            
            print(f"📊 MOCK NOTION SERVICE:")
            print(f"   📋 Formazioni calendarizzate: {len(formazioni_cal)}")
            print(f"   📋 Formazioni completate: {len(formazioni_comp)}")
            
            # Test parsing date dal primo elemento
            if formazioni_cal:
                sample = formazioni_cal[0]
                commands = self.service.commands
                
                data_estratta = commands._extract_date_from_formazione(sample)
                ora_estratta = commands._extract_time_from_formazione(sample)
                giorno = commands._get_day_name(data_estratta) if data_estratta else "N/A"
                
                print(f"\n🔧 TEST PARSING (campione: {sample['Nome']}):")
                print(f"   📅 Data originale: {sample['Data/Ora']}")
                print(f"   📅 Data estratta: {data_estratta}")
                print(f"   🕐 Ora estratta: {ora_estratta}")
                print(f"   📆 Giorno: {giorno}")
            
            # Test filtro per data
            today = datetime.now().date()
            formazioni_oggi = await self.service.commands._get_formazioni_by_date(today)
            
            print(f"\n📅 FILTRO DATE (oggi {today}):")
            print(f"   📋 Formazioni trovate: {len(formazioni_oggi)}")
            for f in formazioni_oggi:
                print(f"      - {f['Nome']} ({f['Area']}) - {f['Data/Ora']}")
            
            return True
            
        except Exception as e:
            print(f"\n❌ Errore diagnostica: {e}")
            return False
    
    async def test_5_send_feedback_notification(self):
        """Test 5: Invio notifica feedback reale (opzionale)."""
        print("\n" + "="*50)
        print("📋 TEST 5: INVIO FEEDBACK REALE")
        print("="*50)
        
        print("⚠️  ATTENZIONE: Questo test invierà notifiche di FEEDBACK REALI!")
        print("📱 Le notifiche saranno inviate sui gruppi configurati")
        print("🔍 I messaggi saranno marcati [TEST] per identificarli")
        
        confirm = input("\n❓ Vuoi continuare con l'invio feedback? (y/N): ").strip().lower()
        
        if confirm not in ['y', 'yes', 's', 'si']:
            print("⏭️ Invio feedback saltato")
            return True
        
        # Usa dati dalle fixture modificati per feedback
        feedback_data = self.sample_data.copy()
        feedback_data['Nome'] = f'Feedback Test Course {datetime.now().strftime("%H:%M")}'
        feedback_data['Codice'] = f'FB{datetime.now().strftime("%H%M")}'
        feedback_data['Stato/Fase'] = 'Completata'  # Necessario per feedback
        feedback_data['Descrizione'] = 'Test reale invio feedback'
        
        # URL feedback di test
        feedback_url = 'https://forms.office.com/test-feedback-form'
        
        print(f"\n📋 Invio feedback per: {feedback_data['Nome']}")
        print(f"🎯 Area target: {feedback_data['Area']}")
        print(f"🔗 URL feedback: {feedback_url}")
        
        try:
            results = await self.service.send_feedback_notification(feedback_data, feedback_url)
            
            successful = sum(1 for success in results.values() if success)
            total = len(results)
            
            print(f"\n✅ Feedback inviato a {successful}/{total} gruppi")
            print(f"📋 Risultati dettagli:")
            for group, success in results.items():
                status = "✅" if success else "❌"
                print(f"   {status} {group}")
            
            print(f"\n🔍 Controlla i gruppi Telegram per vedere i messaggi feedback!")
            return successful > 0
            
        except Exception as e:
            print(f"\n❌ Errore invio feedback: {e}")
            return False
    
    async def run_all_tests(self):
        """Esegue tutti i test in sequenza."""
        print("🧪 QUICK REAL TEST - Formazing Bot")
        print("="*50)
        print("Questo script testa tutti i componenti reali del bot")
        print("Solo NotionService è mock - tutto il resto è reale!")
        print("Usando fixture pytest per configurazione automatica!")
        print("="*50)
        
        results = []
        
        # Test 1: Formattazione (sempre sicuro)
        results.append(await self.test_1_formatting_only())
        
        # Test 2: Invio notifica formazione (opzionale)
        results.append(await self.test_2_send_real_notification())
        
        # Test 3: Comandi bot (opzionale)
        results.append(await self.test_3_bot_commands())
        
        # Test 4: Diagnostica
        results.append(await self.test_4_mock_data_diagnostic())
        
        # Test 5: Invio feedback (opzionale) - NUOVO!
        results.append(await self.test_5_send_feedback_notification())
        
        # Riassunto finale
        print("\n" + "="*50)
        print("📊 RIASSUNTO TEST")
        print("="*50)
        
        test_names = [
            "🎨 Formattazione messaggi",
            "📤 Invio notifica formazione", 
            "🤖 Comandi bot interattivi",
            "🔍 Diagnostica dati mock",
            "📋 Invio notifica feedback"
        ]
        
        successful = 0
        for i, (name, result) in enumerate(zip(test_names, results)):
            status = "✅ PASS" if result else "❌ FAIL"
            print(f"{status} {name}")
            if result:
                successful += 1
        
        print(f"\n🎯 RISULTATO FINALE: {successful}/{len(results)} test superati")
        
        if successful == len(results):
            print("🎉 Tutti i test completati con successo!")
            print("🚀 Il sistema è pronto per l'uso!")
        else:
            print("⚠️ Alcuni test hanno fallito - Controlla i log sopra")


@pytest.mark.asyncio
@pytest.mark.real_telegram  # Marker per test che inviano messaggi reali
async def test_quick_interactive(configured_telegram_service, mock_notion_service, sample_training_data):
    """
    Test interattivo principale che usa le fixture pytest.
    
    Args:
        configured_telegram_service: TelegramService configurato (fixture)
        mock_notion_service: MockNotionService (fixture)
        sample_training_data: Dati di esempio (fixture)
    """
    try:
        # Crea runner con servizi dalle fixture
        runner = QuickTestRunner(
            telegram_service=configured_telegram_service,
            notion_service=mock_notion_service, 
            sample_data=sample_training_data
        )
        
        # Esegui tutti i test
        await runner.run_all_tests()
        
    except KeyboardInterrupt:
        print("\n⌨️ Test interrotto dall'utente")
    except Exception as e:
        print(f"\n❌ Errore generale: {e}")
        pytest.fail(f"Test fallito: {e}")


# Entry point per esecuzione diretta (compatibilità)
if __name__ == "__main__":
    print("💡 Per eseguire questo test, usa:")
    print("   pytest tests/quick_real_test.py::test_quick_interactive -s")
    print("   oppure")
    print("   python -m pytest tests/quick_real_test.py::test_quick_interactive -s")